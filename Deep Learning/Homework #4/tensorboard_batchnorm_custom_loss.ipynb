{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "tensorboard_batchnorm_custom_loss_ED.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gtsVff59QHAW"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRz8ORyCm4Ea"
      },
      "source": [
        "\n",
        "\n",
        "If running with Colab:\n",
        "\n",
        "We commented the cells containing: %tensorboard --logdir runs\n",
        "Since sometimes it got stack. While running the notebook cell by cell we removed the comment, viewed the results and then comment the cell again and rerun it.\n",
        "\n",
        "Another option was to run all the notebook at once and than open the tensorboard window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLEW-40eQHAR"
      },
      "source": [
        "\n",
        "# Image Classification - Tensorboard, Batch Norm and Custom Loss Functions\n",
        "In this exercise, you'll continue to work with our neural network for classifying Israeli Politicians.  \n",
        "We will use tensorboard to monitor the training process and model performance.  \n",
        "\n",
        "For the questions below, please use the network architecture you suggested in Q8 of HW1.  \n",
        "This time, we provide you with a clean dataset of Israeli Politicians, that doesn't include multiple politicians in the same image, in the folder `data/israeli_politicians_cleaned.zip`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubEvlunJQHAT"
      },
      "source": [
        "## Tensorboard\n",
        "TensorBoard provides visualization and tooling for machine learning experimentation:\n",
        "- Tracking and visualizing metrics such as loss and accuracy\n",
        "- Visualizing the model graph (ops and layers)\n",
        "- Viewing histograms of weights, biases, or other tensors as they change over time\n",
        "- Projecting embeddings to a lower dimensional space\n",
        "- Displaying images, text, and audio data\n",
        "- Profiling programs\n",
        "\n",
        "Tensorboard worked originally with Tensorflow but can now be used with PyTorch as well.  \n",
        "You can embed a tensorboard widget in a Jupyter Notebook, although if you're not using Google Colab we recommend that you open tensorboard separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utak4Ro3QHAU"
      },
      "source": [
        "To get started with Tensorboard, please read the following pages:\n",
        "\n",
        "PyTorch related:\n",
        "1. https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html\n",
        "1. https://becominghuman.ai/logging-in-tensorboard-with-pytorch-or-any-other-library-c549163dee9e\n",
        "1. https://towardsdatascience.com/https-medium-com-dinber19-take-a-deeper-look-at-your-pytorch-model-with-the-new-tensorboard-built-in-513969cf6a72\n",
        "1. https://pytorch.org/docs/stable/tensorboard.html\n",
        "1. https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard\n",
        "\n",
        "Tensorflow related:\n",
        "1. https://itnext.io/how-to-use-tensorboard-5d82f8654496\n",
        "1. https://www.datacamp.com/community/tutorials/tensorboard-tutorial\n",
        "1. https://medium.com/@anthony_sarkis/tensorboard-quick-start-in-5-minutes-e3ec69f673af\n",
        "1. https://www.guru99.com/tensorboard-tutorial.html\n",
        "1. https://www.youtube.com/watch?time_continue=1&v=s-lHP8v9qzY&feature=emb_logo\n",
        "1. https://www.youtube.com/watch?v=pSexXMdruFM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nmT6tmFQHAV"
      },
      "source": [
        "### Starting Tensorboard\n",
        "Jupyter Notebook has extensions for displaying TensorBoard inside the notebook. Still, I recommend that you run it separately, as it tends to get stuck in notebooks.\n",
        "\n",
        "The syntax to load TensorBoard in a notebook is this:\n",
        "```python\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVbXcE1aQHAV"
      },
      "source": [
        "In the shell, you can instead run:\n",
        "```\n",
        "tensorboard --logdir ./logs\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-12T03:22:21.503129Z",
          "start_time": "2020-03-12T03:22:18.807319Z"
        },
        "id": "9ecV4HEHQHAV"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfpZUVGavLme"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh8hxDYBQHAW"
      },
      "source": [
        "### Show images using TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaF8HKvkEk-v"
      },
      "source": [
        "from torchvision import datasets, models, transforms\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psn8KLJgFEtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09aab28d-b3cf-4c45-da38-38fa53366e3a"
      },
      "source": [
        "# Create a folder for our data\n",
        "!mkdir data\n",
        "!mkdir data/israeli_politicians_cleaned"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘data/israeli_politicians_cleaned’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5uUu-vhFJqI"
      },
      "source": [
        "# Download our dataset and extract it\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "\n",
        "url = 'https://github.com/omriallouche/ydata_deep_learning_2021/blob/main/data/israeli_politicians_cleaned.zip?raw=true'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('./data/israeli_politicians_cleaned.zip', 'wb').write(r.content)\n",
        "\n",
        "with ZipFile('./data/israeli_politicians_cleaned.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall(path='./data/israeli_politicians_cleaned/')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hggiHTE1E3jZ"
      },
      "source": [
        "# Create transformers\n",
        "means = [0.485, 0.456, 0.406]\n",
        "stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds)\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEetmLkrESZ0",
        "outputId": "57ea58a9-4758-4ef7-b2a8-c1c8b0aba496"
      },
      "source": [
        "# Load data\n",
        "data_dir = r'./data/israeli_politicians_cleaned/'\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=16,\n",
        "                                             shuffle=True, num_workers=4),\n",
        "    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=16,\n",
        "                                          shuffle=False, num_workers=4)\n",
        "  }\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "print('dataset_sizes: ', dataset_sizes)\n",
        "\n",
        "class_names = image_datasets['train'].classes\n",
        "print('class_names:', class_names)\n",
        "\n",
        "trainloader = dataloaders['train']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset_sizes:  {'train': 812, 'val': 202}\n",
            "class_names: ['ayelet_shaked', 'benjamin_netanyahu', 'benny_gantz', 'danny_danon', 'gideon_saar', 'kostya_kilimnik', 'naftali_bennett', 'ofir_akunis', 'yair_lapid']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcEZ_otqHxQi"
      },
      "source": [
        "# writer for tensorboaed\n",
        "\n",
        "# clear logs\n",
        "!rm -rf runs\n",
        "\n",
        "# default `log_dir` is \"runs\"\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SXc5plhLQHT"
      },
      "source": [
        "# Undo normalization to show the original images on Tensorboard\n",
        "def denormalize(image):\n",
        "  inp = image.numpy().transpose((1, 2, 0))\n",
        "  mean = np.array(means)\n",
        "  std = np.array(stds)\n",
        "  inp = std * inp + mean\n",
        "  inp = np.clip(inp, 0, 1)\n",
        "  inp = inp.transpose((2, 0, 1))\n",
        "  return torch.tensor(inp)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-12T03:22:25.305404Z",
          "start_time": "2020-03-12T03:22:22.367810Z"
        },
        "id": "pkxBvlb5QHAW"
      },
      "source": [
        "# Show images using TessorBoard\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "images_to_show = []\n",
        "\n",
        "for i in range(len(images)):\n",
        "  images_to_show.append(denormalize(images[i]))\n",
        "\n",
        "# create grid of images\n",
        "img_grid = torchvision.utils.make_grid(images_to_show)\n",
        "\n",
        "# write to tensorboard\n",
        "writer.add_image('israeli_politicians_cleaned', img_grid)\n",
        "\n",
        "writer.flush()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10a-E58yJVA_"
      },
      "source": [
        "#%tensorboard --logdir runs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtsVff59QHAW"
      },
      "source": [
        "### Inspect the model graph\n",
        "You can print a network object to find useful information about it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEdkJo837KWP"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCicqry57GqP"
      },
      "source": [
        "# Our network from assignment 1\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 3 input image channel, 128 output channels, 3x3 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        \n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(in_features=128*8*8, out_features=1024, bias=True)\n",
        "        self.fc2 = nn.Linear(in_features=1024, out_features=1024, bias=True)\n",
        "        self.fc3 = nn.Linear(in_features=1024, out_features=9, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        x = F.max_pool2d(F.relu(self.conv3(x)), kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        x = F.max_pool2d(F.relu(self.conv4(x)), kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        x = (F.max_pool2d(F.relu(self.conv5(x)), kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x=F.relu(self.fc1(x))\n",
        "        x=F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-12T03:39:13.437035Z",
          "start_time": "2020-03-12T03:39:13.433000Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nurv8MgQHAX",
        "outputId": "cec54b00-8919-4471-92fa-f290441295c4"
      },
      "source": [
        "print(net)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=8192, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc3): Linear(in_features=1024, out_features=9, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eptcUqULQHAX"
      },
      "source": [
        "TensorBoard can help visualize the network graph. It takes practice to read these.  \n",
        "\n",
        "Write the graph to TensorBoard and review it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-12T03:22:27.132650Z",
          "start_time": "2020-03-12T03:22:27.080267Z"
        },
        "id": "a-9d2yJ2QHAX"
      },
      "source": [
        "writer.add_graph(net, images)\n",
        "writer.flush()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oavt8VoBK5_e"
      },
      "source": [
        "#%tensorboard --logdir runs"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Vpn4kNQHAX"
      },
      "source": [
        "You can also use the package `torchsummary` for a fuller info on the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-12T03:39:34.927085Z",
          "start_time": "2020-03-12T03:39:30.883979Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btzoLmipQHAX",
        "outputId": "7cf7a63a-918e-46cf-b147-9b8c1876863e"
      },
      "source": [
        "!pip install torchsummary"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlfWIdwfa2pN"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = net.to(device)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-12T03:40:32.330002Z",
          "start_time": "2020-03-12T03:40:32.304145Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUgmr9R7QHAY",
        "outputId": "64fa7ee0-e8fc-4d15-c485-28b49aa7a3d1"
      },
      "source": [
        "channels=3; H=256; W=256\n",
        "from torchsummary import summary\n",
        "summary(net, input_size=(channels, H, W))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
            "            Conv2d-2         [-1, 64, 128, 128]          36,928\n",
            "            Conv2d-3          [-1, 128, 64, 64]          73,856\n",
            "            Conv2d-4          [-1, 128, 32, 32]         147,584\n",
            "            Conv2d-5          [-1, 128, 16, 16]         147,584\n",
            "            Linear-6                 [-1, 1024]       8,389,632\n",
            "            Linear-7                 [-1, 1024]       1,049,600\n",
            "            Linear-8                    [-1, 9]           9,225\n",
            "================================================================\n",
            "Total params: 9,856,201\n",
            "Trainable params: 9,856,201\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 45.27\n",
            "Params size (MB): 37.60\n",
            "Estimated Total Size (MB): 83.61\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChFSKJDwQHAZ"
      },
      "source": [
        "## Train the network\n",
        "Next, we'll train the network. In the training loop, log relevant metrics that would allow you to plot in TensorBoard:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQErCUiSQHAZ"
      },
      "source": [
        "1. The network loss\n",
        "1. Train and test error\n",
        "1. Average weight in the first layer\n",
        "1. Histogram of weights in the first layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPGCLqjHbyFv"
      },
      "source": [
        "import time\n",
        "import copy"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-12T02:25:56.483936Z",
          "start_time": "2020-03-12T02:17:35.780050Z"
        },
        "id": "yG66YjwgQHAZ"
      },
      "source": [
        "# this version write logs for tensorBoard\n",
        "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    # Init variables that will save info about the best model\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                # Set model to training mode. \n",
        "                model.train()  \n",
        "            else:\n",
        "                # Set model to evaluate mode. In evaluate mode, we don't perform backprop and don't need to keep the gradients\n",
        "                model.eval()   \n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                # Prepare the inputs for GPU/CPU\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # ===== forward pass ======\n",
        "                with torch.set_grad_enabled(phase=='train'):\n",
        "                    # If we're in train mode, we'll track the gradients to allow back-propagation\n",
        "                    outputs = model(inputs) # apply the model to the inputs. The output is the softmax probability of each class\n",
        "                    _, preds = torch.max(outputs, 1) # \n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # ==== backward pass + optimizer step ====\n",
        "                    # This runs only in the training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward() # Perform a step in the opposite direction of the gradient\n",
        "                        optimizer.step() # Adapt the optimizer\n",
        "\n",
        "                # Collect statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "            if phase == 'train':\n",
        "                # Adjust the learning rate based on the scheduler\n",
        "                scheduler.step()  \n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # log for tensorBoard\n",
        "            writer.add_scalar(f'Loss/{phase}', epoch_loss, epoch) # loss\n",
        "            writer.add_scalar(f'Error/{phase}', 1 - epoch_acc, epoch) # error\n",
        "            if phase == 'train':\n",
        "              writer.add_histogram(\"conv1.weight\", model.conv1.weight, epoch) # layer 1 weights histogram\n",
        "              writer.add_scalar('conv1.weight.avg', torch.mean(model.conv1.weight), epoch) # layer 1 weights average\n",
        "\n",
        "            # Keep the results of the best model so far\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                # deepcopy the model\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {(time_elapsed // 60):.0f}m {(time_elapsed % 60):.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwP6UI9bXb-N",
        "outputId": "01a1384d-3816-4639-9460-79c7880b869b"
      },
      "source": [
        "# train the network using the same parameters as in assignment 1 which gave the best performance for our network\n",
        "\n",
        "optimizer_ft = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=2,\n",
        "                                             shuffle=True, num_workers=4),\n",
        "    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=2,\n",
        "                                          shuffle=False, num_workers=4)\n",
        "  }\n",
        "\n",
        "net = train_model(net, \n",
        "                    dataloaders,\n",
        "                       criterion, \n",
        "                       optimizer_ft, \n",
        "                       exp_lr_scheduler,\n",
        "                       num_epochs=20)\n",
        "writer.flush()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 2.1015 Acc: 0.2672\n",
            "val Loss: 2.0875 Acc: 0.2376\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 2.0375 Acc: 0.2820\n",
            "val Loss: 2.0910 Acc: 0.2376\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 2.0096 Acc: 0.2820\n",
            "val Loss: 2.0806 Acc: 0.2574\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 1.9712 Acc: 0.2796\n",
            "val Loss: 2.0372 Acc: 0.2376\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 1.9063 Acc: 0.3153\n",
            "val Loss: 1.9549 Acc: 0.2723\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 1.8531 Acc: 0.3510\n",
            "val Loss: 1.9496 Acc: 0.3267\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 1.7982 Acc: 0.3559\n",
            "val Loss: 1.9383 Acc: 0.3020\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 1.6191 Acc: 0.4138\n",
            "val Loss: 1.8283 Acc: 0.3465\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 1.5445 Acc: 0.4594\n",
            "val Loss: 1.8148 Acc: 0.3515\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 1.4845 Acc: 0.4828\n",
            "val Loss: 1.8660 Acc: 0.3663\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 1.4152 Acc: 0.5000\n",
            "val Loss: 1.7650 Acc: 0.4010\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 1.3249 Acc: 0.5456\n",
            "val Loss: 2.0607 Acc: 0.3614\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 1.2286 Acc: 0.5776\n",
            "val Loss: 1.8754 Acc: 0.4010\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 1.1317 Acc: 0.6219\n",
            "val Loss: 1.7873 Acc: 0.4752\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 0.8715 Acc: 0.7438\n",
            "val Loss: 1.7577 Acc: 0.4851\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 0.8115 Acc: 0.7574\n",
            "val Loss: 1.7920 Acc: 0.4752\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 0.7692 Acc: 0.7648\n",
            "val Loss: 1.8279 Acc: 0.4406\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 0.7328 Acc: 0.7808\n",
            "val Loss: 1.7788 Acc: 0.4752\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 0.7024 Acc: 0.7980\n",
            "val Loss: 1.8244 Acc: 0.4703\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 0.6623 Acc: 0.8091\n",
            "val Loss: 1.8504 Acc: 0.4752\n",
            "\n",
            "Training complete in 1m 54s\n",
            "Best val Acc: 0.485149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMt_wB3baPdi"
      },
      "source": [
        "#%tensorboard --logdir runs"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-c3HzRjQHAZ"
      },
      "source": [
        "### Precision-Recall Curve\n",
        "Use TensorBoard to plot the precision-recall curve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUiOGWyyxXoK"
      },
      "source": [
        "# returns all predictions, labels and probabilities\n",
        "def get_pred_with_probs(model, data_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    real_values = []\n",
        "    probabilities = []\n",
        "    sm = torch.nn.Softmax()\n",
        "\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = sm(model(inputs))\n",
        "            probs, preds = torch.max(outputs, 1)\n",
        "            predictions.extend(preds)\n",
        "            real_values.extend(labels)\n",
        "            probabilities.extend(outputs)\n",
        "  \n",
        "    predictions = torch.as_tensor(predictions).cpu()\n",
        "    real_values = torch.as_tensor(real_values).cpu()\n",
        "    probabilities = torch.stack(probabilities)\n",
        "    return predictions, real_values, probabilities"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcY0SSoXQHAZ"
      },
      "source": [
        "# write PR Curve\n",
        "\n",
        "y_pred, y_test, probs = get_pred_with_probs(net, dataloaders['val'])\n",
        "classes = range(9)\n",
        "\n",
        "for i in classes:\n",
        "  labels_i = y_test == i\n",
        "  preds_i = probs[:, i]\n",
        "  writer.add_pr_curve(class_names[i], labels_i, preds_i, global_step=0)\n",
        "\n",
        "writer.flush()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPtBW20axhI7"
      },
      "source": [
        "#%tensorboard --logdir runs"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEA9I0ibQHAa"
      },
      "source": [
        "### Display Model Errors\n",
        "A valuable practice is to review errors made by the model in the test set. These might reveal cases of bad preprocessing or lead to come up with improvements to your original model.\n",
        "\n",
        "Show 12 images of errors made by the model. For each, display the true and predicted classes, and the model confidence in its answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzQIyezeQHAa"
      },
      "source": [
        "errors = np.array(y_pred != y_test)\n",
        "err_probs = errors / np.count_nonzero(errors)\n",
        "indices = np.random.choice(len(errors),size=12,replace=False,p=err_probs)\n",
        "max_probs, _ = probs.max(axis=1)\n",
        "\n",
        "for i in indices:\n",
        "    label = f'True label: {class_names[y_test[i]]}  Predicted label: {class_names[y_pred[i]]}  Confidence: {max_probs[i]}'\n",
        "    image = denormalize(dataloaders['val'].dataset[i][0])\n",
        "    writer.add_image(label, image)\n",
        "\n",
        "writer.flush()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wpdraKZmZ2g"
      },
      "source": [
        "#%tensorboard --logdir runs"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsNsNnb3QHAa"
      },
      "source": [
        "## Batch Normalization\n",
        "In this section, we'll add a Batch Norm layer to your network.  \n",
        "Use TensorBoard to compare the network's convergence (train and validation loss) with and without Batch Normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-12T02:02:02.225508Z",
          "start_time": "2020-03-12T02:02:02.204005Z"
        },
        "id": "6gVK9NGRQHAa"
      },
      "source": [
        "# Added norm after each layer which improved the performance\n",
        "class NetNorm(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NetNorm, self).__init__()\n",
        "        # 3 input image channel, 128 output channels, 3x3 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv1_bn=nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv2_bn=nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv3_bn=nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv4_bn=nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv5_bn=nn.BatchNorm2d(128)\n",
        "        \n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(in_features=128*8*8, out_features=1024, bias=True)\n",
        "        self.fc1_bn=nn.BatchNorm1d(1024)\n",
        "        self.fc2 = nn.Linear(in_features=1024, out_features=1024, bias=True)\n",
        "        self.fc2_bn=nn.BatchNorm1d(1024)\n",
        "        self.fc3 = nn.Linear(in_features=1024, out_features=9, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = self.conv1(x)\n",
        "        x = F.max_pool2d(F.relu(self.conv1_bn(x)), kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        x = self.conv2(x)\n",
        "        x = F.max_pool2d(F.relu(self.conv2_bn(x)), kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        x = self.conv3(x)\n",
        "        x = F.max_pool2d(F.relu(self.conv3_bn(x)), kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        x = self.conv4(x)\n",
        "        x = F.max_pool2d(F.relu(self.conv4_bn(x)), kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        x = self.conv5(x)\n",
        "        x = (F.max_pool2d(F.relu(self.conv5_bn(x)), kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(self.fc1_bn(x))\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(self.fc2_bn(x))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz33b5C41nFa"
      },
      "source": [
        "# train with logging loss and wether the model uses normalizations\n",
        "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, norm=\"NoNorm\"):\n",
        "    since = time.time()\n",
        "\n",
        "    # Init variables that will save info about the best model\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                # Set model to training mode. \n",
        "                model.train()  \n",
        "            else:\n",
        "                # Set model to evaluate mode. In evaluate mode, we don't perform backprop and don't need to keep the gradients\n",
        "                model.eval()   \n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                # Prepare the inputs for GPU/CPU\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # ===== forward pass ======\n",
        "                with torch.set_grad_enabled(phase=='train'):\n",
        "                    # If we're in train mode, we'll track the gradients to allow back-propagation\n",
        "                    outputs = model(inputs) # apply the model to the inputs. The output is the softmax probability of each class\n",
        "                    _, preds = torch.max(outputs, 1) # \n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # ==== backward pass + optimizer step ====\n",
        "                    # This runs only in the training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward() # Perform a step in the opposite direction of the gradient\n",
        "                        optimizer.step() # Adapt the optimizer\n",
        "\n",
        "                # Collect statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "            if phase == 'train':\n",
        "                # Adjust the learning rate based on the scheduler\n",
        "                scheduler.step()  \n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # log for tensorBoard\n",
        "            writer.add_scalar(f'{norm}/Loss/{phase}', epoch_loss, epoch) # loss\n",
        "\n",
        "            # Keep the results of the best model so far\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                # deepcopy the model\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {(time_elapsed // 60):.0f}m {(time_elapsed % 60):.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI7EByCd5aWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dddbcdc8-81d7-4593-f7f6-805ee748ae88"
      },
      "source": [
        "# to compare performance with batchnorm we will set batch_size=16, without norm the net converges very slowly\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=16,\n",
        "                                             shuffle=True, num_workers=4),\n",
        "    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=16,\n",
        "                                          shuffle=False, num_workers=4)\n",
        "  }\n",
        "\n",
        "net = Net()\n",
        "net = net.to(device)\n",
        "optimizer_ft = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "net = train_model(net, \n",
        "                    dataloaders,\n",
        "                       criterion, \n",
        "                       optimizer_ft, \n",
        "                       exp_lr_scheduler,\n",
        "                       num_epochs=20)\n",
        "writer.flush()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 2.1932 Acc: 0.1798\n",
            "val Loss: 2.1806 Acc: 0.2376\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 2.1603 Acc: 0.2820\n",
            "val Loss: 2.1549 Acc: 0.2376\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 2.1186 Acc: 0.2820\n",
            "val Loss: 2.1209 Acc: 0.2376\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 2.0573 Acc: 0.2820\n",
            "val Loss: 2.0985 Acc: 0.2376\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 2.0261 Acc: 0.2820\n",
            "val Loss: 2.0916 Acc: 0.2376\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 2.0180 Acc: 0.2820\n",
            "val Loss: 2.0936 Acc: 0.2376\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 2.0115 Acc: 0.2820\n",
            "val Loss: 2.0766 Acc: 0.2376\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 2.0108 Acc: 0.2820\n",
            "val Loss: 2.0756 Acc: 0.2376\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 2.0022 Acc: 0.2820\n",
            "val Loss: 2.0696 Acc: 0.2376\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 1.9913 Acc: 0.2820\n",
            "val Loss: 2.0518 Acc: 0.2525\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 1.9797 Acc: 0.2919\n",
            "val Loss: 2.0599 Acc: 0.2426\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 1.9569 Acc: 0.2759\n",
            "val Loss: 2.0297 Acc: 0.2475\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 1.9370 Acc: 0.2943\n",
            "val Loss: 2.0020 Acc: 0.2624\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 1.9096 Acc: 0.3030\n",
            "val Loss: 2.0220 Acc: 0.3020\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 1.8657 Acc: 0.3276\n",
            "val Loss: 2.0034 Acc: 0.2871\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 1.8506 Acc: 0.3424\n",
            "val Loss: 2.0014 Acc: 0.2970\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 1.7993 Acc: 0.3633\n",
            "val Loss: 1.9856 Acc: 0.3317\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 1.7270 Acc: 0.3879\n",
            "val Loss: 1.8934 Acc: 0.2871\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 1.6922 Acc: 0.4126\n",
            "val Loss: 1.8522 Acc: 0.3267\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 1.5933 Acc: 0.4470\n",
            "val Loss: 1.9666 Acc: 0.2970\n",
            "\n",
            "Training complete in 1m 27s\n",
            "Best val Acc: 0.331683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0docOUlw3P4u",
        "outputId": "e30aa2ba-99b5-4149-90d8-c6ac40aebd45"
      },
      "source": [
        "# train the model which uses the batch norm with logging the loss\n",
        "netNorm = NetNorm()\n",
        "netNorm = netNorm.to(device)\n",
        "optimizer_ft = optim.SGD(netNorm.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "netNorm = train_model(netNorm, \n",
        "                    dataloaders,\n",
        "                       criterion, \n",
        "                       optimizer_ft, \n",
        "                       exp_lr_scheduler,\n",
        "                       num_epochs=20,\n",
        "                      norm=\"WithNorm\")\n",
        "writer.flush()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 1.8953 Acc: 0.3473\n",
            "val Loss: 1.7155 Acc: 0.4406\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 1.0391 Acc: 0.6909\n",
            "val Loss: 1.3776 Acc: 0.5842\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 0.4818 Acc: 0.9101\n",
            "val Loss: 1.1415 Acc: 0.6485\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 0.2178 Acc: 0.9889\n",
            "val Loss: 1.0435 Acc: 0.6485\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 0.0983 Acc: 0.9988\n",
            "val Loss: 1.0168 Acc: 0.6683\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 0.0561 Acc: 1.0000\n",
            "val Loss: 1.0104 Acc: 0.6782\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 0.0361 Acc: 1.0000\n",
            "val Loss: 1.0005 Acc: 0.6485\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 0.0384 Acc: 1.0000\n",
            "val Loss: 0.9753 Acc: 0.6584\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 0.0257 Acc: 1.0000\n",
            "val Loss: 0.9985 Acc: 0.6634\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 0.0200 Acc: 1.0000\n",
            "val Loss: 1.0227 Acc: 0.6832\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 0.0197 Acc: 1.0000\n",
            "val Loss: 1.0711 Acc: 0.6683\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 0.0159 Acc: 1.0000\n",
            "val Loss: 1.0583 Acc: 0.6683\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 0.0156 Acc: 1.0000\n",
            "val Loss: 1.0598 Acc: 0.6634\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 0.0136 Acc: 1.0000\n",
            "val Loss: 1.0445 Acc: 0.6733\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 0.0093 Acc: 1.0000\n",
            "val Loss: 1.0438 Acc: 0.6683\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 0.0127 Acc: 0.9988\n",
            "val Loss: 1.1072 Acc: 0.6634\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 0.0089 Acc: 1.0000\n",
            "val Loss: 1.0557 Acc: 0.6634\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 0.0080 Acc: 1.0000\n",
            "val Loss: 1.0306 Acc: 0.6683\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 0.0078 Acc: 1.0000\n",
            "val Loss: 1.0476 Acc: 0.6634\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 0.0069 Acc: 1.0000\n",
            "val Loss: 1.0678 Acc: 0.6782\n",
            "\n",
            "Training complete in 1m 39s\n",
            "Best val Acc: 0.683168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ6z0QFm3uvd"
      },
      "source": [
        "#%tensorboard --logdir runs"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLwH5S_gQHAa"
      },
      "source": [
        "Use TensorBoard to plot the distribution of activations with and without Batch Normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSYnfkG4QHAa"
      },
      "source": [
        "activation = {}\n",
        "\n",
        "def get_activation(name):\n",
        "\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "\n",
        "    return hook"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTYP5jOqPJEh"
      },
      "source": [
        "# add activation histogram of each fc layer\n",
        "models = {'No_Norm': Net(),\n",
        "          'Norm': NetNorm()}\n",
        "\n",
        "images = images.to(device)\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model = model.to(device)\n",
        "    \n",
        "    for name, layer in model.named_modules():\n",
        "      if '_bn' not in name and 'fc' in name:\n",
        "        layer.register_forward_hook(get_activation(name))\n",
        "    \n",
        "    outputs = model(images)\n",
        "\n",
        "    for name, layer in model.named_modules():\n",
        "      if '_bn' not in name and 'fc' in name:\n",
        "        act = activation[name].squeeze()\n",
        "        writer.add_histogram(f'{name}/{model_name}', act)\n",
        "    \n",
        "    writer.flush()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQBZKa52fedf"
      },
      "source": [
        "#%tensorboard --logdir runs"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXNnvhhIQHAb"
      },
      "source": [
        "## Custom Loss Function\n",
        "Manually labeled datasets often contain labeling errors. These can have a large effect on the trained model.  \n",
        "In this task we’ll work on a highly noisy dataset. Take our cleaned Israeli Politicians dataset and randomly replace 10% of the true labels.\n",
        "Compare the performance of the original model to a similar model trained on the noisy labels. \n",
        "\n",
        "Suggest a loss function that might help with noisy labels. Following this guide, implement your own custom loss function in PyTorch and compare the model performance using it:  \n",
        "https://discuss.pytorch.org/t/solved-what-is-the-correct-way-to-implement-custom-loss-function/3568/9\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDpqaQugoxdn"
      },
      "source": [
        "# train which returns val accuracy as well\n",
        "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, norm=\"NoNorm\"):\n",
        "    since = time.time()\n",
        "\n",
        "    # Init variables that will save info about the best model\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                # Set model to training mode. \n",
        "                model.train()  \n",
        "            else:\n",
        "                # Set model to evaluate mode. In evaluate mode, we don't perform backprop and don't need to keep the gradients\n",
        "                model.eval()   \n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                # Prepare the inputs for GPU/CPU\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # ===== forward pass ======\n",
        "                with torch.set_grad_enabled(phase=='train'):\n",
        "                    # If we're in train mode, we'll track the gradients to allow back-propagation\n",
        "                    outputs = model(inputs) # apply the model to the inputs. The output is the softmax probability of each class\n",
        "                    _, preds = torch.max(outputs, 1) # \n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # ==== backward pass + optimizer step ====\n",
        "                    # This runs only in the training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward() # Perform a step in the opposite direction of the gradient\n",
        "                        optimizer.step() # Adapt the optimizer\n",
        "\n",
        "                # Collect statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "            if phase == 'train':\n",
        "                # Adjust the learning rate based on the scheduler\n",
        "                scheduler.step()  \n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # Keep the results of the best model so far\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                # deepcopy the model\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {(time_elapsed // 60):.0f}m {(time_elapsed % 60):.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return (model, best_acc)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUiEkCLHruIR",
        "outputId": "9b536509-a169-4b9b-9b0a-673de8c725a6"
      },
      "source": [
        "# NetNorm trained on clean data\n",
        "netNorm = NetNorm()\n",
        "netNorm = netNorm.to(device)\n",
        "optimizer_ft = optim.SGD(netNorm.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "netNorm, net_acc = train_model(netNorm, \n",
        "                    dataloaders,\n",
        "                       criterion, \n",
        "                       optimizer_ft, \n",
        "                       exp_lr_scheduler,\n",
        "                       num_epochs=20,\n",
        "                      norm=\"WithNorm\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 1.8771 Acc: 0.3399\n",
            "val Loss: 1.6735 Acc: 0.4604\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 1.0551 Acc: 0.6921\n",
            "val Loss: 1.4022 Acc: 0.5743\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 0.4780 Acc: 0.9113\n",
            "val Loss: 1.1471 Acc: 0.6733\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 0.2122 Acc: 0.9828\n",
            "val Loss: 1.0862 Acc: 0.6485\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 0.0961 Acc: 0.9975\n",
            "val Loss: 1.0213 Acc: 0.6733\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 0.0613 Acc: 1.0000\n",
            "val Loss: 1.0372 Acc: 0.6683\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 0.0351 Acc: 1.0000\n",
            "val Loss: 1.0389 Acc: 0.6535\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 0.0286 Acc: 1.0000\n",
            "val Loss: 1.0541 Acc: 0.6584\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 0.0228 Acc: 1.0000\n",
            "val Loss: 1.0500 Acc: 0.6683\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 0.0193 Acc: 1.0000\n",
            "val Loss: 1.0690 Acc: 0.6634\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 0.0165 Acc: 1.0000\n",
            "val Loss: 1.0510 Acc: 0.6782\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 0.0147 Acc: 1.0000\n",
            "val Loss: 1.0670 Acc: 0.6634\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 0.0135 Acc: 1.0000\n",
            "val Loss: 1.0776 Acc: 0.6782\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 0.0139 Acc: 1.0000\n",
            "val Loss: 1.0863 Acc: 0.6683\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 0.0119 Acc: 1.0000\n",
            "val Loss: 1.1047 Acc: 0.6782\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 0.0113 Acc: 1.0000\n",
            "val Loss: 1.0583 Acc: 0.6733\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 0.0076 Acc: 1.0000\n",
            "val Loss: 1.0863 Acc: 0.6782\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 0.0084 Acc: 1.0000\n",
            "val Loss: 1.0753 Acc: 0.6733\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 0.0083 Acc: 1.0000\n",
            "val Loss: 1.0772 Acc: 0.6782\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 0.0064 Acc: 1.0000\n",
            "val Loss: 1.0801 Acc: 0.6832\n",
            "\n",
            "Training complete in 1m 40s\n",
            "Best val Acc: 0.683168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOm74kNaQHAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "583e449a-bd95-4ec2-a9fe-9da2c72b8294"
      },
      "source": [
        "# Mess 10% of the labels and train NetNorm model on it\n",
        "num_images = len(dataloaders['train'].dataset)\n",
        "indices = np.random.choice(range(num_images),size=int(num_images/10),replace=False)\n",
        "\n",
        "for i in indices:\n",
        "  probs = np.array([1/8,1/8,1/8,1/8,1/8,1/8,1/8,1/8,1/8])\n",
        "  probs[dataloaders['train'].dataset[i][1]] = 0\n",
        "  dataloaders['train'].dataset.samples[i] = (dataloaders['train'].dataset.samples[i][0],np.random.choice(range(9),p=probs))\n",
        "\n",
        "netNoise = NetNorm()\n",
        "netNoise = netNoise.to(device)\n",
        "optimizer_ft = optim.SGD(netNoise.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "netNoise, noise_acc = train_model(netNoise, \n",
        "                    dataloaders,\n",
        "                       criterion, \n",
        "                       optimizer_ft, \n",
        "                       exp_lr_scheduler,\n",
        "                       num_epochs=20,\n",
        "                      norm=\"WithNorm\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 1.9554 Acc: 0.3091\n",
            "val Loss: 1.7676 Acc: 0.4010\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 1.2125 Acc: 0.6281\n",
            "val Loss: 1.4841 Acc: 0.5644\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 0.6669 Acc: 0.8362\n",
            "val Loss: 1.2729 Acc: 0.5891\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 0.3833 Acc: 0.9273\n",
            "val Loss: 1.2399 Acc: 0.5941\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 0.2314 Acc: 0.9581\n",
            "val Loss: 1.2516 Acc: 0.5990\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 0.1794 Acc: 0.9544\n",
            "val Loss: 1.3061 Acc: 0.6089\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 0.1537 Acc: 0.9631\n",
            "val Loss: 1.2716 Acc: 0.5990\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 0.1596 Acc: 0.9557\n",
            "val Loss: 1.3358 Acc: 0.5941\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 0.1377 Acc: 0.9569\n",
            "val Loss: 1.3053 Acc: 0.6040\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 0.1258 Acc: 0.9618\n",
            "val Loss: 1.3096 Acc: 0.6139\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 0.1169 Acc: 0.9631\n",
            "val Loss: 1.3079 Acc: 0.5941\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 0.0987 Acc: 0.9667\n",
            "val Loss: 1.3255 Acc: 0.5990\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 0.0924 Acc: 0.9643\n",
            "val Loss: 1.3549 Acc: 0.5842\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 0.0977 Acc: 0.9631\n",
            "val Loss: 1.3991 Acc: 0.6040\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 0.0973 Acc: 0.9618\n",
            "val Loss: 1.3572 Acc: 0.5941\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 0.0867 Acc: 0.9643\n",
            "val Loss: 1.4015 Acc: 0.5941\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 0.0721 Acc: 0.9717\n",
            "val Loss: 1.3878 Acc: 0.6188\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 0.0627 Acc: 0.9778\n",
            "val Loss: 1.3838 Acc: 0.6089\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 0.0633 Acc: 0.9741\n",
            "val Loss: 1.4134 Acc: 0.5990\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 0.0665 Acc: 0.9729\n",
            "val Loss: 1.4118 Acc: 0.5792\n",
            "\n",
            "Training complete in 1m 40s\n",
            "Best val Acc: 0.618812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AC3ZMK3vmIg",
        "outputId": "06753700-dc49-45c9-c56b-6f5f7653853d"
      },
      "source": [
        "print(\"Val accuracy without noise:\", net_acc.item())\n",
        "print(\"Val accuracy with noise:\", noise_acc.item())"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val accuracy without noise: 0.6831683168316832\n",
            "Val accuracy with noise: 0.6188118811881188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuseB_UWwSAq"
      },
      "source": [
        "We see a decrease of the validation accuracy when training on noisy data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv5TTzghrLTR"
      },
      "source": [
        "# Boost the cross entropy with reversed cross entropy, we can set the weight assigned to each method\n",
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self, ce_weight, rce_weight, num_classes=9):\n",
        "        super(CustomLoss, self).__init__()\n",
        "        self.device = device\n",
        "        self.ce_weight = ce_weight\n",
        "        self.rce_weight = rce_weight\n",
        "        self.num_classes = num_classes\n",
        "        self.cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, pred, labels):\n",
        "        # Cross entropy loss\n",
        "        ce = self.cross_entropy(pred, labels)\n",
        "\n",
        "        # Reversed cross entropy\n",
        "        pred = F.softmax(pred, dim=1)\n",
        "        pred = torch.clamp(pred, min=1e-7, max=1.0)\n",
        "        label_one_hot = torch.nn.functional.one_hot(labels, self.num_classes).float().to(self.device)\n",
        "        label_one_hot = torch.clamp(label_one_hot, min=1e-4, max=1.0)\n",
        "        rce = (-1*torch.sum(pred * torch.log(label_one_hot), dim=1))\n",
        "\n",
        "        # Loss\n",
        "        loss = self.ce_weight * ce + self.rce_weight * rce.mean()\n",
        "        return loss"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkqIP-KmtzcJ",
        "outputId": "47a59aee-66c0-4f72-d167-2790a12a15fb"
      },
      "source": [
        "# train a model with the new loss function\n",
        "criterion = CustomLoss(0.7,0.3)\n",
        "netNoise = NetNorm()\n",
        "netNoise = netNoise.to(device)\n",
        "optimizer_ft = optim.SGD(netNoise.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "netNoise, new_loss_acc = train_model(netNoise, \n",
        "                    dataloaders,\n",
        "                       criterion, \n",
        "                       optimizer_ft, \n",
        "                       exp_lr_scheduler,\n",
        "                       num_epochs=20,\n",
        "                      norm=\"WithNorm\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 3.5343 Acc: 0.3264\n",
            "val Loss: 3.2422 Acc: 0.3317\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 2.4710 Acc: 0.5628\n",
            "val Loss: 2.6907 Acc: 0.5000\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 1.5092 Acc: 0.7956\n",
            "val Loss: 2.3215 Acc: 0.6188\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 0.8726 Acc: 0.8916\n",
            "val Loss: 2.3398 Acc: 0.5396\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 0.5275 Acc: 0.9372\n",
            "val Loss: 2.3180 Acc: 0.5792\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 0.3765 Acc: 0.9520\n",
            "val Loss: 2.1745 Acc: 0.5990\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 0.2877 Acc: 0.9631\n",
            "val Loss: 2.1656 Acc: 0.6089\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 0.2811 Acc: 0.9569\n",
            "val Loss: 2.1557 Acc: 0.5990\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 0.2539 Acc: 0.9606\n",
            "val Loss: 2.1907 Acc: 0.6238\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 0.2490 Acc: 0.9631\n",
            "val Loss: 2.1496 Acc: 0.6287\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 0.2295 Acc: 0.9594\n",
            "val Loss: 2.1839 Acc: 0.6089\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 0.1959 Acc: 0.9643\n",
            "val Loss: 2.1456 Acc: 0.5990\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 0.2149 Acc: 0.9618\n",
            "val Loss: 2.2195 Acc: 0.6089\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 0.1996 Acc: 0.9618\n",
            "val Loss: 2.2220 Acc: 0.6040\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 0.2069 Acc: 0.9655\n",
            "val Loss: 2.2861 Acc: 0.5792\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 0.1936 Acc: 0.9606\n",
            "val Loss: 2.2611 Acc: 0.5792\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 0.1970 Acc: 0.9655\n",
            "val Loss: 2.2171 Acc: 0.5891\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 0.1694 Acc: 0.9692\n",
            "val Loss: 2.2434 Acc: 0.5941\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 0.1673 Acc: 0.9643\n",
            "val Loss: 2.1635 Acc: 0.6337\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 0.1667 Acc: 0.9680\n",
            "val Loss: 2.2907 Acc: 0.5891\n",
            "\n",
            "Training complete in 1m 41s\n",
            "Best val Acc: 0.633663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlIez_Qg6kiw",
        "outputId": "9ba971b9-27d8-43b8-a370-eb359916c9f2"
      },
      "source": [
        "print(\"Val accuracy with nn.CrossEntropyLoss():\", noise_acc.item())\n",
        "print(\"Val accuracy with the custom loss:\", new_loss_acc.item())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val accuracy with nn.CrossEntropyLoss(): 0.6188118811881188\n",
            "Val accuracy with the custom loss: 0.6336633663366337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwkgRtbbBbJj"
      },
      "source": [
        "For most of the runs the custom loss function improved the noisy data performance. Still it couldn't reach the accuracy of the model trained with no noise."
      ]
    }
  ]
}
