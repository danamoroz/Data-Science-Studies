{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2_2021 DE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpXmBjIHDyk0"
      },
      "source": [
        "\n",
        "## Building a deep learning calculator\n",
        "\n",
        "In this HW we will use seq2seq for building a calculator. The input will be an equation and the solution will be generated by the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yUUpCXrD5Tv"
      },
      "source": [
        "### Data Generation\n",
        "\n",
        "In this task we will generate our own data! We will use three operators (addition, multiplication and subtraction) and work with positive integer numbers in some range. Here are examples of correct inputs and outputs:\n",
        "\n",
        "    Input: '1+2'\n",
        "    Output: '3'\n",
        "    \n",
        "    Input: '0-99'\n",
        "    Output: '-99'\n",
        "\n",
        "*Note, that there are no spaces between operators and operands.*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Ps0hhd3p6k"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import time\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1oF_FI43p6o"
      },
      "source": [
        "def generate_equations(allowed_operators, dataset_size, min_value, max_value):\n",
        "    \"\"\"Generates pairs of equations and solutions to them.\n",
        "    \n",
        "       Each equation has a form of two integers with an operator in between.\n",
        "       Each solution is an integer with the result of the operaion.\n",
        "    \n",
        "        allowed_operators: list of strings, allowed operators.\n",
        "        dataset_size: an integer, number of equations to be generated.\n",
        "        min_value: an integer, min value of each operand.\n",
        "        max_value: an integer, max value of each operand.\n",
        "\n",
        "        result: a list of tuples of strings (equation, solution).\n",
        "    \"\"\"\n",
        "    sample = []\n",
        "    for _ in range(dataset_size):\n",
        "        left_operand = str(random.randint(min_value, max_value))\n",
        "        right_operand = str(random.randint(min_value, max_value))\n",
        "        operator = random.choice(allowed_operators)\n",
        "        operation = left_operand+operator+right_operand\n",
        "        sample.append((operation, str(eval(operation))))\n",
        "    return sample"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_NL2hsKEGOk"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "allowed_operators = ['+', '-','*']\n",
        "dataset_size = 100000\n",
        "data = generate_equations(allowed_operators, dataset_size, min_value=0, max_value=10000)\n",
        "\n",
        "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK4MK0ulEe57",
        "outputId": "c247d3e3-488d-4661-df54-d7e566bec8fb"
      },
      "source": [
        "train_set[0:10]"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('3254*1672', '5440688'),\n",
              " ('392-4831', '-4439'),\n",
              " ('1640+2270', '3910'),\n",
              " ('604-6924', '-6320'),\n",
              " ('6183-5508', '675'),\n",
              " ('3570*4747', '16946790'),\n",
              " ('545*7237', '3944165'),\n",
              " ('7994*1627', '13006238'),\n",
              " ('6407*3606', '23103642'),\n",
              " ('7627+9222', '16849')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UPW3sV8lrsb"
      },
      "source": [
        "### Building vocabularies and tokenization function\n",
        "\n",
        "We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these  when we feed training data into  the model or convert output matrices into words. \n",
        "\n",
        "Pay a close attention to the special characters you need to add for the vocabulary:\n",
        "\n",
        "\n",
        "*    End of equation / solution token\n",
        "*    Begining of equation / solution token\n",
        "*    Padding token\n",
        "\n",
        "Please note that in the exercise we do not need the <UNK> token\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmTy_m_olrsb"
      },
      "source": [
        "class Vocab():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.chars = ['0','1','2','3','4','5','6','7','8','9','+','-','*','END','BEGIN','PAD']\n",
        "\n",
        "    #build a vocabulary  string --> tokenId\n",
        "    self.char_to_ix = { ch:i for i,ch in enumerate(self.chars) }\n",
        "\n",
        "    #build a reverse vocabulary  tokenId --> string\n",
        "    self.ix_to_char = { i:ch for i,ch in enumerate(self.chars) }\n",
        "\n",
        "    self.eos_ix = self.char_to_ix['END']\n",
        "    self.bos_ix = self.char_to_ix['BEGIN']\n",
        "    self.pad_ix = self.char_to_ix['PAD']\n",
        "    self.max_len = 13\n",
        "\n",
        "  #build a tokenizer (i.e. a function which takes a string and returns tokenids)\n",
        "  def tokenize(self, data):\n",
        "    \n",
        "    data = list(data)\n",
        "    tokens = [self.char_to_ix[tok] for tok in data]\n",
        "    tokens = [self.bos_ix] + tokens + [self.eos_ix]\n",
        "    pads = []\n",
        "    padLen = self.max_len - len(tokens)\n",
        "\n",
        "    for p in range(padLen):\n",
        "      pads.append(self.pad_ix)\n",
        "    \n",
        "    return torch.tensor(tokens + pads, dtype=torch.long, device=device).view(-1, 1)\n",
        "  \n",
        "  def __len__(self):\n",
        "        return len(self.chars)\n",
        "\n",
        "  def tensorsFromPair(self, pair):\n",
        "    input_tensor = self.tokenize(pair[0])\n",
        "    target_tensor = self.tokenize(pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "vocab = Vocab()"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHWgx34flrsn"
      },
      "source": [
        "### Encoder-decoder model\n",
        "\n",
        "The code below contains a template for a simple encoder-decoder model: single GRU encoder/decoder.\n",
        "**Please note that some places require change and your implementation.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bTKTwj95way"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxpTijQR59oc"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQDhGwg4lrtC"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Training encoder-decoder models isn't that different from any other models: sample batches, compute loss, backprop and update.\n",
        "\n",
        "For training loss we will use ***torch.nn.NLLLoss*** please note that the loss should not be calculated with the padding token. (For ignoring specific labels please look at ***torch.nn.NLLLoss*** documentation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nW0e2xDRuzj"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "MAX_LENGTH = 13\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH, attention=False):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[vocab.bos_ix]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            \n",
        "            if attention:\n",
        "              decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                  decoder_input, decoder_hidden, encoder_outputs)\n",
        "            \n",
        "            else:\n",
        "              decoder_output, decoder_hidden = decoder(\n",
        "                  decoder_input, decoder_hidden)\n",
        "              \n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            \n",
        "            if attention:\n",
        "              decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                  decoder_input, decoder_hidden, encoder_outputs)\n",
        "              \n",
        "            else:\n",
        "              decoder_output, decoder_hidden = decoder(\n",
        "                  decoder_input, decoder_hidden)\n",
        "\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == vocab.eos_ix:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F1pQJI0QNwl"
      },
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDbBv8kyR_AA"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01, attention=False):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [vocab.tensorsFromPair(random.choice(train_set))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion, attention=attention)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_sevsDjSY0k"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH, attention=False):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = vocab.tokenize(sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[vocab.bos_ix]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "\n",
        "            if attention:\n",
        "              decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                  decoder_input, decoder_hidden, encoder_outputs)\n",
        "              decoder_attentions[di] = decoder_attention.data\n",
        "              \n",
        "            else:\n",
        "              decoder_output, decoder_hidden = decoder(\n",
        "                  decoder_input, decoder_hidden)\n",
        "              \n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == vocab.eos_ix:\n",
        "                decoded_words.append('END')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(vocab.ix_to_char[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjOsy26iUVWN"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10, attention=False):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(test_set)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0],attention=attention)\n",
        "        output_words = output_words[1:-1]\n",
        "        output_sentence = ''.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRabXkhuTHTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa0073e-8a46-4d47-d5a7-bca861485d84"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(len(vocab), hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, len(vocab)).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 90000, print_every=5000)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 18s (- 22m 16s) (5000 5%) 1.0297\n",
            "2m 33s (- 20m 26s) (10000 11%) 1.0133\n",
            "3m 48s (- 19m 2s) (15000 16%) 1.0023\n",
            "5m 3s (- 17m 43s) (20000 22%) 0.9830\n",
            "6m 20s (- 16m 29s) (25000 27%) 0.9689\n",
            "7m 37s (- 15m 15s) (30000 33%) 0.9413\n",
            "8m 54s (- 14m 0s) (35000 38%) 0.9261\n",
            "10m 10s (- 12m 43s) (40000 44%) 0.9078\n",
            "11m 26s (- 11m 26s) (45000 50%) 0.9022\n",
            "12m 42s (- 10m 10s) (50000 55%) 0.8917\n",
            "13m 58s (- 8m 53s) (55000 61%) 0.8775\n",
            "15m 13s (- 7m 36s) (60000 66%) 0.8685\n",
            "16m 29s (- 6m 20s) (65000 72%) 0.8663\n",
            "17m 44s (- 5m 4s) (70000 77%) 0.8678\n",
            "19m 0s (- 3m 48s) (75000 83%) 0.8527\n",
            "20m 15s (- 2m 31s) (80000 88%) 0.8470\n",
            "21m 31s (- 1m 15s) (85000 94%) 0.8374\n",
            "22m 47s (- 0m 0s) (90000 100%) 0.8353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0dNESswTvJR",
        "outputId": "01db7a30-8091-4e47-8906-5386746a1a43"
      },
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> 6883+3527\n",
            "= 10410\n",
            "< 10138\n",
            "\n",
            "> 3780+4380\n",
            "= 8160\n",
            "< 9988\n",
            "\n",
            "> 6690+7299\n",
            "= 13989\n",
            "< 14395\n",
            "\n",
            "> 8885+9846\n",
            "= 18731\n",
            "< 17878\n",
            "\n",
            "> 9056+9473\n",
            "= 18529\n",
            "< 17758\n",
            "\n",
            "> 9880+7500\n",
            "= 17380\n",
            "< 17758\n",
            "\n",
            "> 4770-3946\n",
            "= 824\n",
            "< 208\n",
            "\n",
            "> 1337+3664\n",
            "= 5001\n",
            "< 4988\n",
            "\n",
            "> 9037*2303\n",
            "= 20812211\n",
            "< 11423959\n",
            "\n",
            "> 1399*2948\n",
            "= 4124252\n",
            "< 4407588\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz9aROAIlrtX"
      },
      "source": [
        "## Adding Attention Layer\n",
        "Here you will have to implement a layer that computes a simple additive attention:\n",
        "\n",
        "Given encoder sequence $ h^e_0, h^e_1, h^e_2, ..., h^e_T$ and a single decoder state $h^d$,\n",
        "\n",
        "* Compute logits with a 2-layer neural network\n",
        "$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
        "* Get probabilities from logits, \n",
        "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
        "\n",
        "* Add up encoder states with probabilities to get __attention response__\n",
        "$$ attn = \\sum_t p_t \\cdot h^e_t $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH688oqxYeOc"
      },
      "source": [
        "#<implement attention layer>\n",
        "\n",
        "class AdditiveAttention(torch.nn.Module):\t \t \n",
        "    def __init__(self, encoder_dim=256, decoder_dim=256):\t \t \n",
        "        super().__init__()\t \t \n",
        "\n",
        "        self.encoder_dim = encoder_dim\t \t \n",
        "        self.decoder_dim = decoder_dim\t \t \n",
        "        self.v = torch.nn.Parameter(torch.rand(self.decoder_dim))\t \t \n",
        "        self.W_1 = torch.nn.Linear(self.decoder_dim, self.decoder_dim)\t \t \n",
        "        self.W_2 = torch.nn.Linear(self.encoder_dim, self.decoder_dim)\t \t \n",
        "\n",
        "    def forward(self, query, values):\t \t \n",
        "        weights = self._get_weights(query, values) \n",
        "        weights = torch.nn.functional.softmax(weights, dim=0)\n",
        "        return weights @ values, weights \n",
        "\n",
        "    def _get_weights(self, \t \t \n",
        "      query, # [decoder_dim]\t \t \n",
        "      values # [seq_length, encoder_dim]\t \t \n",
        "    ):\t\n",
        "        query = query.repeat(values.size(0), 1) # [seq_length, decoder_dim]\t \t \n",
        "        weights = self.W_1(query) + self.W_2(values) # [seq_length, decoder_dim]\t \t \n",
        "        return torch.tanh(weights) @ self.v # [seq_length]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk9SOZaYYqAw"
      },
      "source": [
        "#<add attention layer for your seq2seq model>\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.attention = AdditiveAttention()\n",
        "        self.dropout_p = dropout_p\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # compute context vector using attention mechanism\n",
        "        query = hidden.squeeze()\n",
        "        context, attn_probs = self.attention(\n",
        "            query=query, values=encoder_outputs)\n",
        "        embedded = embedded.to(device)\n",
        "        embedded = embedded.squeeze().unsqueeze(0)\n",
        "        context = context.unsqueeze(0)\n",
        "        output = torch.cat((embedded, context), 1)\n",
        "        output = self.attn_combine(output)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output.unsqueeze(0), hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden, attn_probs\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAVabTrPZPTj",
        "outputId": "14e7469a-b04e-422b-b6b0-a16a06a0da64"
      },
      "source": [
        "#<Train the two models (with/without attention and compare the results)\n",
        "\n",
        "hidden_size = 256\n",
        "encoder2 = EncoderRNN(len(vocab), hidden_size).to(device)\n",
        "decoder2 = AttnDecoderRNN(hidden_size, len(vocab)).to(device)\n",
        "\n",
        "trainIters(encoder2, decoder2, 90000, print_every=5000, attention=True)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2m 3s (- 34m 57s) (5000 5%) 1.0301\n",
            "4m 3s (- 32m 31s) (10000 11%) 1.0089\n",
            "6m 3s (- 30m 19s) (15000 16%) 0.9755\n",
            "8m 4s (- 28m 17s) (20000 22%) 0.9402\n",
            "10m 5s (- 26m 14s) (25000 27%) 0.9280\n",
            "12m 7s (- 24m 14s) (30000 33%) 0.9093\n",
            "14m 9s (- 22m 15s) (35000 38%) 0.8841\n",
            "16m 10s (- 20m 13s) (40000 44%) 0.8656\n",
            "18m 11s (- 18m 11s) (45000 50%) 0.8550\n",
            "20m 12s (- 16m 10s) (50000 55%) 0.8446\n",
            "22m 13s (- 14m 8s) (55000 61%) 0.8371\n",
            "24m 15s (- 12m 7s) (60000 66%) 0.8407\n",
            "26m 16s (- 10m 6s) (65000 72%) 0.8295\n",
            "28m 17s (- 8m 4s) (70000 77%) 0.8299\n",
            "30m 17s (- 6m 3s) (75000 83%) 0.8145\n",
            "32m 18s (- 4m 2s) (80000 88%) 0.8220\n",
            "34m 19s (- 2m 1s) (85000 94%) 0.8128\n",
            "36m 19s (- 0m 0s) (90000 100%) 0.8039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR-ODvwz6bzD",
        "outputId": "5fb95d78-78a6-4962-8616-337645a3cda0"
      },
      "source": [
        "evaluateRandomly(encoder2, decoder2, attention=True)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> 8365+6290\n",
            "= 14655\n",
            "< 14399\n",
            "\n",
            "> 7367*850\n",
            "= 6261950\n",
            "< 6630880\n",
            "\n",
            "> 9356-797\n",
            "= 8559\n",
            "< 8889\n",
            "\n",
            "> 1917+7788\n",
            "= 9705\n",
            "< 9036\n",
            "\n",
            "> 6783+1076\n",
            "= 7859\n",
            "< 7768\n",
            "\n",
            "> 454*9207\n",
            "= 4179978\n",
            "< 4330088\n",
            "\n",
            "> 6857-6053\n",
            "= 804\n",
            "< 166\n",
            "\n",
            "> 8382*2620\n",
            "= 21960840\n",
            "< 20000800\n",
            "\n",
            "> 8804*1283\n",
            "= 11295532\n",
            "< 12896998\n",
            "\n",
            "> 6125*4505\n",
            "= 27593125\n",
            "< 20055680\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v63Jds6lY8iD"
      },
      "source": [
        "## Good Luck!\n",
        "Thanks :) We can see some improvement with using attention"
      ]
    }
  ]
}
